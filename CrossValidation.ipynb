{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Cross-Validation is a statistical technique used to evaluate the performance and generalizability of a machine learning model. It involves partitioning the dataset into multiple subsets or \"folds\" to test the model's performance on different sections of the data, ensuring that the model is not overfitting or underfitting. The main goal of cross-validation is to estimate how well the model will perform on unseen data by using different subsets for training and testing.\n",
        "\n",
        "## **Applications of Cross-Validation**\n",
        "- **Model Evaluation**: Provides a robust estimate of a model's performance by testing it on multiple data subsets.\n",
        "- **Hyperparameter Tuning**: Identifies the best hyperparameters by assessing model performance across different configurations.\n",
        "- **Model Comparison**: Ensures fair evaluation of different models by using the same data partitions.\n",
        "- **Reducing Overfitting**: Detects overfitting by validating the model on various subsets, revealing its performance on unseen data.\n",
        "- **Feature Selection**: Assesses the impact of different features on model performance to determine their relevance.\n",
        "- **Estimating Model Stability**: Evaluates how consistent a model’s performance is across different data subsets.\n"
      ],
      "metadata": {
        "id": "Blx1cgGfZgBe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## Lets discuss and code each type of CrossValidation"
      ],
      "metadata": {
        "id": "1llkScnwtaSM"
      },
      "execution_count": 74,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Leave-One-Out Cross-Validation (LOOCV)**\n",
        "\n",
        "**Description**:  \n",
        "In LOOCV, a single observation is used as the validation set, and the remaining observations form the training set. This process is repeated for each observation in the dataset.\n",
        "\n",
        "**Advantages**:\n",
        "- Uses as much data as possible for training.\n",
        "- Low bias since almost all data points are used for training.\n",
        "\n",
        "**Disadvantages**:\n",
        "- Computationally expensive, especially for large datasets.\n",
        "- High variance as each training set is almost identical.\n",
        "\n",
        "**Use Cases**:\n",
        "- Small datasets where retaining more data for training is crucial.\n",
        "- When high accuracy is required and computational cost is not a constraint."
      ],
      "metadata": {
        "id": "iogm4YgFl02k"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "RI5eR5dZY0ny"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import LeaveOneOut\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.metrics import accuracy_score"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load data\n",
        "data = load_iris()\n",
        "X, y = data.data, data.target"
      ],
      "metadata": {
        "id": "2Y0cRyFsaC5E"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize the model\n",
        "model = LogisticRegression(max_iter=200)"
      ],
      "metadata": {
        "id": "bYlPBeT0aFQd"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize LOOCV\n",
        "loo = LeaveOneOut()\n",
        "accuracy = []"
      ],
      "metadata": {
        "id": "g0iN-7HmaMT7"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Perform LOOCV\n",
        "for train_index, test_index in loo.split(X):\n",
        "    X_train, X_test = X[train_index], X[test_index]\n",
        "    y_train, y_test = y[train_index], y[test_index]\n",
        "\n",
        "    # Train the model\n",
        "    model.fit(X_train, y_train)\n",
        "\n",
        "    # Make predictions\n",
        "    y_pred = model.predict(X_test)\n",
        "\n",
        "    # Evaluate the model\n",
        "    accuracy.append(accuracy_score(y_test, y_pred))"
      ],
      "metadata": {
        "id": "NpaxfZj8aMRa"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate average accuracy\n",
        "print(f'Average Accuracy: {sum(accuracy)/len(accuracy):.4f}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pyVmrQLnaMOb",
        "outputId": "fcd8c53a-336a-4caf-ceef-c329df02bec2"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average Accuracy: 0.9667\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Hold-Out Cross-Validation**\n",
        "\n",
        "**Description**:  \n",
        "The dataset is randomly split into two parts: a training set and a validation set. The model is trained on the training set and evaluated on the validation set.\n",
        "\n",
        "**Advantages**:\n",
        "- Simple to implement.\n",
        "- Fast, since the model is trained and validated only once.\n",
        "\n",
        "**Disadvantages**:\n",
        "- High variance as the results depend heavily on the split.\n",
        "- Potential for overfitting if the split does not represent the overall dataset well.\n",
        "\n",
        "**Use Cases**:\n",
        "- Large datasets where a single split is representative.\n",
        "- Quick evaluation of models.\n"
      ],
      "metadata": {
        "id": "-A2ndBVqaWt7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.ensemble import RandomForestClassifier"
      ],
      "metadata": {
        "id": "zZiFCVfhaMDL"
      },
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate some example data\n",
        "X = np.random.rand(100, 10)  # 100 samples, 10 features\n",
        "y = np.random.randint(0, 2, 100)  # Binary target variable"
      ],
      "metadata": {
        "id": "Z3am84dwaixS"
      },
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
      ],
      "metadata": {
        "id": "6rZolTW4qwGN"
      },
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize and train the model\n",
        "model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "model.fit(X_train, y_train)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 75
        },
        "id": "8qRtCfQyalCy",
        "outputId": "3158e473-6436-4f53-8350-5f470eb80020"
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "RandomForestClassifier(random_state=42)"
            ],
            "text/html": [
              "<style>#sk-container-id-3 {color: black;}#sk-container-id-3 pre{padding: 0;}#sk-container-id-3 div.sk-toggleable {background-color: white;}#sk-container-id-3 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-3 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-3 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-3 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-3 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-3 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-3 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-3 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-3 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-3 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-3 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-3 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-3 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-3 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-3 div.sk-item {position: relative;z-index: 1;}#sk-container-id-3 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-3 div.sk-item::before, #sk-container-id-3 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-3 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-3 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-3 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-3 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-3 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-3 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-3 div.sk-label-container {text-align: center;}#sk-container-id-3 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-3 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-3\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>RandomForestClassifier(random_state=42)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-3\" type=\"checkbox\" checked><label for=\"sk-estimator-id-3\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">RandomForestClassifier</label><div class=\"sk-toggleable__content\"><pre>RandomForestClassifier(random_state=42)</pre></div></div></div></div></div>"
            ]
          },
          "metadata": {},
          "execution_count": 48
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Make predictions\n",
        "y_pred = model.predict(X_test)"
      ],
      "metadata": {
        "id": "XRFpWTZhan1y"
      },
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate the model\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f'Accuracy: {accuracy:.2f}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_5Be11-eaqUi",
        "outputId": "70f54d18-d3a2-4abd-ee81-0ab76307f1ab"
      },
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.60\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**K-Fold Cross-Validation**\n",
        "\n",
        "**Description**:  \n",
        "The dataset is divided into K subsets or \"folds.\" The model is trained on K-1 folds and validated on the remaining fold. This process is repeated K times, with each fold used exactly once as the validation set.\n",
        "\n",
        "**Advantages**:\n",
        "- Reduces variance as each data point is used for both training and validation.\n",
        "- Provides a more comprehensive evaluation of the model.\n",
        "\n",
        "**Disadvantages**:\n",
        "- Computationally more expensive than hold-out cross-validation.\n",
        "- Can still be computationally expensive for very large datasets.\n",
        "\n",
        "**Use Cases**:\n",
        "- When the dataset is not very large.\n",
        "- To obtain a reliable estimate of model performance.\n"
      ],
      "metadata": {
        "id": "Ub8f6yXkmJBa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.ensemble import RandomForestClassifier"
      ],
      "metadata": {
        "id": "B6opYZ9_a4ba"
      },
      "execution_count": 62,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate some example data\n",
        "X = np.random.rand(100, 10)  # 100 samples, 10 features\n",
        "y = np.random.randint(0, 2, 100)  # Binary target variable"
      ],
      "metadata": {
        "id": "lFtFho4Ia_iI"
      },
      "execution_count": 63,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize the model\n",
        "model = RandomForestClassifier(n_estimators=100, random_state=42)"
      ],
      "metadata": {
        "id": "eIsJGyPUsGuc"
      },
      "execution_count": 64,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the number of folds\n",
        "k = 5\n",
        "kf = KFold(n_splits=k, shuffle=True, random_state=42)"
      ],
      "metadata": {
        "id": "HpoENcFKbBx7"
      },
      "execution_count": 65,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Lists to store results\n",
        "fold_accuracies = []"
      ],
      "metadata": {
        "id": "sv4PbT_ZbHCO"
      },
      "execution_count": 66,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Perform K-Fold Cross-Validation\n",
        "for train_index, test_index in kf.split(X):\n",
        "    X_train, X_test = X[train_index], X[test_index]\n",
        "    y_train, y_test = y[train_index], y[test_index]\n",
        "\n",
        "    # Train the model\n",
        "    model.fit(X_train, y_train)\n",
        "\n",
        "    # Make predictions\n",
        "    y_pred = model.predict(X_test)\n",
        "\n",
        "    # Evaluate the model\n",
        "    accuracy = accuracy_score(y_test, y_pred)\n",
        "    fold_accuracies.append(accuracy)"
      ],
      "metadata": {
        "id": "O06RZDZ7slWu"
      },
      "execution_count": 67,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate the average accuracy across all folds\n",
        "average_accuracy = np.mean(fold_accuracies)\n",
        "print(f'Average Accuracy: {average_accuracy:.2f}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PpjWRFmPslUM",
        "outputId": "0e729cbf-adeb-4f1f-864e-174afab2ccbb"
      },
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average Accuracy: 0.51\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Stratified K-Fold Cross-Validation**\n",
        "\n",
        "**Description**:  \n",
        "Stratified K-Fold Cross-Validation is similar to K-Fold, but it ensures that each fold has the same proportion of classes as the entire dataset. This is particularly useful for imbalanced datasets.\n",
        "\n",
        "**Advantages**:\n",
        "- Maintains the proportion of classes in each fold.\n",
        "- More accurate performance estimation for imbalanced datasets.\n",
        "\n",
        "**Disadvantages**:\n",
        "- Slightly more complex to implement than regular K-Fold.\n",
        "- Computationally more expensive than hold-out cross-validation.\n",
        "\n",
        "**Use Cases**:\n",
        "- Imbalanced datasets where maintaining class distribution is important.\n",
        "- Situations where model evaluation needs to consider class proportions.\n"
      ],
      "metadata": {
        "id": "26wHvRaxbOCj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import StratifiedKFold"
      ],
      "metadata": {
        "id": "X_V4h42dbJwJ"
      },
      "execution_count": 69,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize Stratified K-Fold with 5 folds\n",
        "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
        "accuracy = []"
      ],
      "metadata": {
        "id": "W9-aZp5KbVVi"
      },
      "execution_count": 70,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load data\n",
        "data = load_iris()\n",
        "X, y = data.data, data.target"
      ],
      "metadata": {
        "id": "Vh1Nole5sIPU"
      },
      "execution_count": 71,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Perform Stratified K-Fold CV\n",
        "for train_index, test_index in skf.split(X, y):\n",
        "    X_train, X_test = X[train_index], X[test_index]\n",
        "    y_train, y_test = y[train_index], y[test_index]\n",
        "\n",
        "    # Train the model\n",
        "    model.fit(X_train, y_train)\n",
        "\n",
        "    # Make predictions\n",
        "    y_pred = model.predict(X_test)\n",
        "\n",
        "    # Evaluate the model\n",
        "    accuracy.append(accuracy_score(y_test, y_pred))"
      ],
      "metadata": {
        "id": "yKSj2n3jbVRF"
      },
      "execution_count": 72,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate average accuracy\n",
        "print(f'Average Accuracy: {sum(accuracy)/len(accuracy):.4f}')"
      ],
      "metadata": {
        "id": "_eIk2OYIbVPM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "80cb2a23-0c4f-4651-9d83-2c90cdfc95d2"
      },
      "execution_count": 73,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average Accuracy: 0.9467\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Time Series Cross-Validation**\n",
        "\n",
        "**Description**:  \n",
        "Time Series Cross-Validation is used for time series data, where the temporal order of the data must be preserved. It involves using past data to predict future data by progressively expanding the training set.\n",
        "\n",
        "**Advantages**:\n",
        "- Respects the temporal order of data.\n",
        "- Suitable for time series forecasting tasks.\n",
        "\n",
        "**Disadvantages**:\n",
        "- Cannot shuffle data, which may lead to lower variability in training data.\n",
        "- May result in less training data for early folds.\n",
        "\n",
        "**Use Cases**:\n",
        "- Time series data where temporal order is critical.\n",
        "- Forecasting tasks where training on past data to predict future data is necessary.\n"
      ],
      "metadata": {
        "id": "FjaqXuMgbiao"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.model_selection import TimeSeriesSplit"
      ],
      "metadata": {
        "id": "PtX-cm0UbVLi"
      },
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate some example time series data\n",
        "np.random.seed(42)\n",
        "n_samples = 100\n",
        "X = np.random.rand(n_samples, 1)  # 100 samples, 1 feature\n",
        "y = np.sin(np.linspace(0, 10, n_samples)) + np.random.randn(n_samples) * 0.1  # Example target variable"
      ],
      "metadata": {
        "id": "DxbQ8_QvbVJa"
      },
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize Time Series Split with 5 splits\n",
        "tscv = TimeSeriesSplit(n_splits=5)\n",
        "mse = []"
      ],
      "metadata": {
        "id": "QdI4E-yLbwbO"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize the model\n",
        "model = RandomForestRegressor(n_estimators=100, random_state=42)"
      ],
      "metadata": {
        "id": "xpnDAlkcnDx8"
      },
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the number of splits\n",
        "n_splits = 5\n",
        "tscv = TimeSeriesSplit(n_splits=n_splits)"
      ],
      "metadata": {
        "id": "NdSnB6yWbwZC"
      },
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Lists to store results\n",
        "fold_mse = []"
      ],
      "metadata": {
        "id": "FW-2MOB3bwWn"
      },
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Perform Time Series Cross-Validation\n",
        "for train_index, test_index in tscv.split(X):\n",
        "    X_train, X_test = X[train_index], X[test_index]\n",
        "    y_train, y_test = y[train_index], y[test_index]\n",
        "\n",
        "    # Train the model\n",
        "    model.fit(X_train, y_train)\n",
        "\n",
        "    # Make predictions\n",
        "    y_pred = model.predict(X_test)\n",
        "\n",
        "    # Evaluate the model\n",
        "    mse = mean_squared_error(y_test, y_pred)\n",
        "    fold_mse.append(mse)"
      ],
      "metadata": {
        "id": "G53uXmDlpxFG"
      },
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate the average Mean Squared Error across all folds\n",
        "average_mse = np.mean(fold_mse)\n",
        "print(f'Average Mean Squared Error: {average_mse:.2f}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MRpivA6er6P8",
        "outputId": "7f758b21-3425-4edf-dd2a-2e9704c5253e"
      },
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average Mean Squared Error: 0.83\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "bQd2Y0qTr8vb"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}